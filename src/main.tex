% This program is free software: you can redistribute it and/or modify
% it under the terms of the GNU AFFERO General Public License as published by
% the Free Software Foundation, either version 3 of the License, or
% (at your option) any later version.
% 
% This program is distributed in the hope that it will be useful,
% but WITHOUT ANY WARRANTY; without even the implied warranty of
% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
% GNU General Public License for more details.
% 
% You should have received a copy of the GNU AFFERO General Public License
% along with this program.  If not, see <https://www.gnu.org/licenses/>.
%
% Copyright (C) 2020 Mo Zhou <cdluminate@gmail.com>
%
\documentclass[9pt,twocolumn,times]{article}
\usepackage{times}
\usepackage[margin=0.9in]{geometry}
\usepackage{tikz}
\usepackage{pgflibraryshapes}
\usetikzlibrary{arrows.meta}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{mathtools}
\input{include/math_commands.tex}

\title{Computation Graph Cheat Sheet}
\author{Mo Zhou\\\small\texttt{<cdluminate@gmail.com>}\\
License: AGPL-3.0}

\begin{document}

\maketitle
\tableofcontents

\section{Conventions}

Vectors are column vectors by default.
We follow the naming convention of BLAS/LAPACK for linear algebra operations.
Vecotr, Matrix, Tensor shapes are annotated under the corresponding symbols.

\section{Automatic Differentiation}

\section{Atomic Modules}

\subsection{Level1: Vector-Vector Ops}

	\textbf{Identity}

	\textbf{Addition} (Add)

	\textbf{Element-wise Multiplication} (Mul)

	\textbf{Dot Product} (Dot)

	\textbf{Element-wise Inverse} (Inv)

	\textbf{L-2 Norm} (L-2)

	\textbf{Rectified Linear Unit} (ReLU)

	\textbf{Sigmoid} ($\sigma$)

\subsection{Level2: Matrix-Vector Ops}

	\textbf{GEneral Matrix-Vector multiplication} (GEMV)

	Forward Pass:
	\begin{equation}
		\underset{(m\times k)}{\mW} \cdot \underset{(k)}{\vx} =
		\underset{(m)}{\vy}
	\end{equation}

	\begin{figure}[h]
		\centering
		\resizebox{0.618\columnwidth}{!}{%
			\input{tikz/cg-mv.tex}
		}
		\caption{GEMV: $\mW \vx = \vy$. (Linear)}
	\end{figure}

	Backward Pass:
	\begin{align}
		\underset{(m\times k)}{\frac{\partial L}{\partial\mW}} &=
		\underset{(m)}{\frac{\partial L}{\partial\vy}} \cdot
		\underset{(1\times k)}{\vx^T}\\
		\underset{(k)}{\frac{\partial L}{\partial\vx}} &=
		\underset{(k\times m)}{\mW^T} \cdot
		\underset{(m)}{\frac{\partial L}{\partial \vy}}
	\end{align}

\subsection{Level3: Matrix-Matrix Ops}

	\textbf{GEneral Matrix-Matrix multiplication} (GEMM)

	Forward Pass:
	\begin{equation}
		\underset{(m\times k)}{\mW} \cdot
		\underset{(k\times n)}{\mX} =
		\underset{(m\times n)}{\mY}
	\end{equation}

	\begin{figure}[h]
		\centering
		\resizebox{0.618\columnwidth}{!}{%
			\input{tikz/cg-mm.tex}
		}
		\caption{GEMM: $\mW \mX = \mY$. (Linear)}
	\end{figure}

	Backward Pass:
	\begin{align}
		\underset{(m\times k)}{\frac{\partial L}{\partial\mW}} &=
		\underset{(m\times n)}{\frac{\partial L}{\partial\mY}} \cdot
		\underset{(n,k)}{\mX^T}\\
		\underset{(k\times n)}{\frac{\partial L}{\partial\mX}} &=
		\underset{(k\times m)}{\mW^T} \cdot
		\underset{(m\times n)}{\frac{\partial L}{\partial \mY}}
	\end{align}

	\textbf{2-D Convolution} (Conv2d)

\section{Network Layers}

	\textbf{Affine Transformation Layer} (Affine)

	\textbf{Mean Square Error} (MSE)

	Forward Pass:
	\begin{equation}
		\frac{1}{H} \sum_{i=1}^H
		\|\underset{(n)}{\vx_i} - \underset{(n)}{\hat{\vx}_i} \|_2^2
		= \underset{(1)}{L} 
	\end{equation}

	\begin{figure}[h]
		\centering
		\resizebox{0.618\columnwidth}{!}{%
			\input{tikz/cg-mse.tex}
		}
		\caption{MSE: $\frac{1}{H} \sum_{i=1}^H \|\vx_i - \hat{\vx}_i\|_2^2 = L$.}
	\end{figure}

	\textbf{Softmax Layer} (Softmax)

	\textbf{Batch Normalization} (BN)

\section{Networks}

\subsection{Convolutional Neural Networks}

\subsection{Recurrent Neural Networks}

\end{document}
